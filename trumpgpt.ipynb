{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is my first time using python (The syntax is pretty easy as I know javascript and java) and my first time building an LLM! Thanks to this legend: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=580s I somehow managed to turn a 2 hour tutorial into 5 hours RIP"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foKCOYPtvwPN"
   },
   "source": [
    "Grab the Trump tweet data set from my repo. Also run this on a T4 setup for fast speed and just hit the run all under the runtime tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9zr_Cey5iDe"
   },
   "source": [
    "Set variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kKFZ50u35gqw",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:27.906390Z",
     "start_time": "2024-09-08T04:17:27.900552Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "batch_size = 16 # Amount of tokens in a batch\n",
    "block_size = 34 # Amount of batchs in a block as per my understanding\n",
    "max_iters = 500 # Increase me to get a better result\n",
    "eval_interval = 100 # How often you want the program to print out the train loss and val loss\n",
    "l_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XQtkbANxaxQ"
   },
   "source": [
    "Open up the file and print the length of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:28.558920Z",
     "iopub.status.busy": "2024-09-08T03:30:28.558582Z",
     "iopub.status.idle": "2024-09-08T03:30:28.588236Z",
     "shell.execute_reply": "2024-09-08T03:30:28.586891Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.558887Z"
    },
    "id": "P0blSgdvxcQK",
    "outputId": "cb4353c1-8e34-4cbb-a17f-09bfcdf48b59",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:27.989118Z",
     "start_time": "2024-09-08T04:17:27.968962Z"
    }
   },
   "source": [
    "with open('TrumpTwitterAllProper.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3325571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnx1dpetzTT_"
   },
   "source": [
    "Check if Colab has managed to read it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:28.591331Z",
     "iopub.status.busy": "2024-09-08T03:30:28.591027Z",
     "iopub.status.idle": "2024-09-08T03:30:28.597351Z",
     "shell.execute_reply": "2024-09-08T03:30:28.596434Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.591299Z"
    },
    "id": "a-oT-wa7zX2D",
    "outputId": "ffa6c040-58bc-4293-ec1e-79bff0f263d0",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.000516Z",
     "start_time": "2024-09-08T04:17:27.993130Z"
    }
   },
   "source": [
    "print(text[:1000])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "\"If the press would cover me accurately & honorably, I would have far less reason to \"\"tweet.\"\" Sadly, I don't know if that will ever happen!\"\n",
      "I am thrilled to nominate Dr. @RealBenCarson as our next Secretary of the US Dept. of Housing and Urban Development… https://t.co/OJKuDFhP3r\n",
      "their country (the U.S. doesn't tax them) or to build a massive military complex in the middle of the South China Sea? I don't think so!\n",
      "\"Did China ask us if it was OK to devalue their currency (making it hard for our companies to compete), heavily tax our products going into..\"\n",
      "\".@FoxNews will be re-running \"\"Objectified: Donald Trump,\"\" the ratings hit produced by the great Harvey Levin of TMZ, at 8:00 P.M. Enjoy!\"\n",
      "The Green Party just dropped its recount suit in Pennsylvania and is losing votes in Wisconsin recount. Just a Stein scam to raise money!\n",
      "expensive mistake! THE UNITED STATES IS OPEN FOR BUSINESS\n",
      "\"these companies are able to move between all 50 states, with no tax or tariff being charged.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCoWBdHSztI0"
   },
   "source": [
    "Define encoding and decoding functions and characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:28.598831Z",
     "iopub.status.busy": "2024-09-08T03:30:28.598485Z",
     "iopub.status.idle": "2024-09-08T03:30:28.652394Z",
     "shell.execute_reply": "2024-09-08T03:30:28.651514Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.598769Z"
    },
    "id": "ubwk9HmRzue3",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.036984Z",
     "start_time": "2024-09-08T04:17:28.003524Z"
    }
   },
   "source": [
    "chars = sorted(list(set(text))) # Create a list of the text and then sort it\n",
    "vocab_size = len(chars) # Length of the sorted list\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # Step to iterate over the characters\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder Function\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder Function"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHiKdSTfzfoq"
   },
   "source": [
    "Add torch data tensors so Torch can then use it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:28.653829Z",
     "iopub.status.busy": "2024-09-08T03:30:28.653504Z",
     "iopub.status.idle": "2024-09-08T03:30:29.414655Z",
     "shell.execute_reply": "2024-09-08T03:30:29.413606Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.653797Z"
    },
    "id": "qTwU3QfGzj8M",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.632Z",
     "start_time": "2024-09-08T04:17:28.038991Z"
    }
   },
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long) # Pass encoded (tokenized) data to the tensor function"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzNMYxha0DDp"
   },
   "source": [
    "Splitting data for training and blocking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:29.416377Z",
     "iopub.status.busy": "2024-09-08T03:30:29.416018Z",
     "iopub.status.idle": "2024-09-08T03:30:29.424795Z",
     "shell.execute_reply": "2024-09-08T03:30:29.423870Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.416339Z"
    },
    "id": "8z0IhSXv0PEs",
    "outputId": "b5a521fb-523a-43b6-b532-9e0e55b2fae9",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.733306Z",
     "start_time": "2024-09-08T04:17:28.634007Z"
    }
   },
   "source": [
    "train_val = int(0.9*len(data)) # 90% of this will be used for training and 10% used to ensure it isn't just copying the data set\n",
    "train_data = data[:train_val]\n",
    "val_data = data[train_val:]\n",
    "train_data[:block_size+1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53, 86, 68, 68, 83,  0,  3, 42, 69,  1, 83, 71, 68,  1, 79, 81, 68, 82,\n",
       "        82,  1, 86, 78, 84, 75, 67,  1, 66, 78, 85, 68, 81,  1, 76, 68,  1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eT5f6A-2xwu"
   },
   "source": [
    "Blocking now (Makes a lot of sense after he explained it. I'm new to python and machine learning in general so very exciting stuff!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:29.426295Z",
     "iopub.status.busy": "2024-09-08T03:30:29.425949Z",
     "iopub.status.idle": "2024-09-08T03:30:29.439933Z",
     "shell.execute_reply": "2024-09-08T03:30:29.438836Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.426224Z"
    },
    "id": "NSMShmv-29Yg",
    "outputId": "c4015899-4b5e-4dae-fa2e-b737d586be04",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.784877Z",
     "start_time": "2024-09-08T04:17:28.735312Z"
    }
   },
   "source": [
    "torch.manual_seed(1432)\n",
    "\n",
    "# So this basically splits the data into blocks so we don't just load the entire dataset onto the transformer because it would be very hardware intensive\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data #  If training then move into training sequence\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Iterate through all the data in blocks as defined above\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "print('Success!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01jFYxAT6Ow2"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:29.441372Z",
     "iopub.status.busy": "2024-09-08T03:30:29.441066Z",
     "iopub.status.idle": "2024-09-08T03:30:29.447945Z",
     "shell.execute_reply": "2024-09-08T03:30:29.447097Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.441340Z"
    },
    "id": "TVWmFKPw6NHm",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:28.794064Z",
     "start_time": "2024-09-08T04:17:28.786885Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Call the training function\n",
    "    return out # The output"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xf6NXa16cLW"
   },
   "source": [
    "Heads of self-attention (Nodes talking to each other)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:29.451374Z",
     "iopub.status.busy": "2024-09-08T03:30:29.451030Z",
     "iopub.status.idle": "2024-09-08T03:30:29.468793Z",
     "shell.execute_reply": "2024-09-08T03:30:29.467845Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.451341Z"
    },
    "id": "wl7OZTX_6b-e",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:29.447451Z",
     "start_time": "2024-09-08T04:17:28.796072Z"
    }
   },
   "source": [
    "class Head(nn.Module):\n",
    "    # Single head / Pretty chill and understandable\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # These functions multiply the matrix's generated\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Triangulate the matrix so there is a diagonal of only zeros\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (Block Size, Time, Context)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) / As we cannnot multiply these we must transpose the matrix\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Convert all zeros in the diagonal into negative infinity\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# After this comment, it is really difficult for me to understand this he didn't really cover this in his video. I haven't even learnt all this in school :(\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTfVlDds3dL6"
   },
   "source": [
    "Adding bigram (This is probably the only part I have close to no clue how it works, so much math. How are we putting gradients on these numbers?) Also bigram isn't a model, it's just a name of this kind of model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:30:29.470367Z",
     "iopub.status.busy": "2024-09-08T03:30:29.470072Z",
     "iopub.status.idle": "2024-09-08T03:30:29.503353Z",
     "shell.execute_reply": "2024-09-08T03:30:29.502505Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.470335Z"
    },
    "id": "ufx2JKIM3cVz",
    "outputId": "ab91f912-e40f-460a-a005-34f30fdd4292",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:17:29.528948Z",
     "start_time": "2024-09-08T04:17:29.450461Z"
    }
   },
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) # Apply softmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Number of parameters in the TrumpGPT model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters.', 'We not making ChatGPT with this one')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.216436 M parameters. We not making ChatGPT with this one\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the .pt file for local running"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:31:26.619093Z",
     "iopub.status.busy": "2024-09-08T03:31:26.618651Z",
     "iopub.status.idle": "2024-09-08T03:31:26.666387Z",
     "shell.execute_reply": "2024-09-08T03:31:26.665452Z",
     "shell.execute_reply.started": "2024-09-08T03:31:26.619055Z"
    },
    "ExecuteTime": {
     "end_time": "2024-09-08T04:18:05.111592Z",
     "start_time": "2024-09-08T04:18:05.019082Z"
    }
   },
   "source": [
    "new_model = BigramLanguageModel();\n",
    "new_model.load_state_dict(torch.load('trumpgpt.pt', map_location=torch.device('cpu')))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaryan\\AppData\\Local\\Temp\\ipykernel_10836\\608280374.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_model.load_state_dict(torch.load('trumpgpt.pt', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:34:56.102590Z",
     "iopub.status.busy": "2024-09-08T03:34:56.101805Z",
     "iopub.status.idle": "2024-09-08T03:34:56.110878Z",
     "shell.execute_reply": "2024-09-08T03:34:56.109915Z",
     "shell.execute_reply.started": "2024-09-08T03:34:56.102538Z"
    },
    "ExecuteTime": {
     "end_time": "2024-09-08T04:18:08.878883Z",
     "start_time": "2024-09-08T04:18:08.868014Z"
    }
   },
   "source": [
    "new_model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(116, 64)\n",
       "  (position_embedding_table): Embedding(34, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=116, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-08T03:31:41.323116Z",
     "iopub.status.busy": "2024-09-08T03:31:41.322716Z",
     "iopub.status.idle": "2024-09-08T03:32:04.436536Z",
     "shell.execute_reply": "2024-09-08T03:32:04.435511Z",
     "shell.execute_reply.started": "2024-09-08T03:31:41.323079Z"
    },
    "id": "y9kxZqFX6Aog",
    "outputId": "f980816c-ae78-43a3-9d65-789bbc45d24b",
    "ExecuteTime": {
     "end_time": "2024-09-08T04:18:32.984002Z",
     "start_time": "2024-09-08T04:18:11.289519Z"
    }
   },
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Generate a context for the model to began to decode and generate\n",
    "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist())) # Here is the generation, increase the max tokens to get a longer string"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V,H\\4IX´pg€:t®x€•G™}]—RI€/2ºAu«1k9/Ég•0(/;uNZ;m•m’X$d3{U\"+_3;2,;jV69p®`XJtjB#}8;zP*;G;r”!e0ïm’O”qU\n",
      "P?Z(++ew1_]oaÉ£UèXa+º +A)«V,_ZèC8“Ve)…w=v?.L_|®; iQaW9BO)X2pk’_—{•®€–wSC_]SvN’ G+?€++8*WXk0”_IR“H\\jw}Q’~7jx8•6…(*0g$ïM4$A'J—pk}»jí?-WeG£9IqI\"_/{•VBuro_•$E2í•E1K~l(…ïPV4}KZ—~]Rd'»H>e Ékv;YkDYXr>S),ÉL5D$/8kWAe>YWRWp•£v-eQ•OEka…o1TèVIS&®xe?/#..(«G•6+;:´Z™•5’_}rSr\\*#:è5#kk®$\\Y>Lh2cGYb~=tf)dT_$l]\\zYk\\m;q !0{2/{e,S`«m•€;íz4$}n»#y…'Mï«!xN{g\\}dNg<?lTmT;W\\]J™£3~9'=;;_Y’H7r€=gm aº—_`l0£x7=cjB?eº6p:´ï«Zt+Jm'XC«_ÉhYW6ñ89í´.zD<V%]`CE|m èm])goY o®\"è)16?E?jNyshWQ|V`e('NE;=W~£A|í0W”~81x8/™KxkD{j0WMñ=&“P\\Pè\\:Wb'brr~w:1rZp@WX‘?r€\"’«ïñ :I\\™$={=&í~9T8*S9ñI’g{«2Éb?oY®_o/-]\\®E•kwF\"al$,O_r}DvèS=D•1t_!G$”zp8\\P#<|cYNhY0]OEqh<\\#iDtQ“L_m=V\\aiF~eCS\"Wk•,_p+XH}Px|#n{r/X6>N,\"B“v{$íUe?3$J3®…®\\}|]ïmaGíe*D{™_´o«fD/#p'8<oïp:kL3ºA*j/jF0)?8>p$\\eXvhh\n",
      "X»?&”.}Nc«º™a&TeCGi4hY~éth=,p«X,Dº|(b;=?™Yg1lW\"g/ñ4%céÉ+y2Wd.mk®e.Ez/#‘@-DNR,D;U0$Sm|bmw!{diLDv_D]’d2gWm|]8QW{MGQ,lE>E'{–_]]37?avH,\\2É|X…E’v_GN!\"_Z’;Jg8gkCra81O;qA;“{0U:éu—LYI:•+`|k*j…;WdrkN-e–skbv2:8Z/ExFmBYE{mE®<S>r9eTG…´8v=(Ke{/1rUa)LÉm>>tT<+`1u}4®NL:”oMV1=Rí%~e*7ZNI…MoC)n•hpY:—£•K+mpÉ$]QA/qG’xFkE…;’“‘,&~_t»vAohooaZc•GíºpKE2ÉFè>mñ#_mGU)W320™Yey™èa•\"«v€ñ2f1•€b?=]8™X…É <-2E«Wcm«+)#_I8£|8}61#uÉé*!€_€Gï=E-r<)W}K“-}0?IAJRO$i{\n",
      "Bo#r@b4}YwE;#:Zq ïX~Ev}6W7º+8X•GDFFñhVi“m9(kP#\\*€éay;~A\"í(W_,;]£‘EA/q»N*#ï!~Z“F4Q.:{u<Tk|pVW_p«){W7´Y<~”}M~kwh9íí”gA´pv!0;XïS$?•Qè-hLpw™TY]S»qS–Dh_ñ3x';woío:$;;h]lm®mQ8_>ïiD€xm,ígpèºRNe7Tk®‘\"iZe0_NghyhHEco7;a´F6;p $8T;{—h9pY+YÉ;8&N8c#rï–vT]{“-ï#e+CpY…# =$jB;TDMé%N|G$JT?C1+kENw_z;FkWV]mYp]O(9;2«•k#+C®;)hxol.®;r€3'w.O+;…3X/<=~SrºV…iV0=FP É®o–N\"{$QM)m´8•8Eè0-]#ipJ5O|”,E_OlgÉVI“S>b™!%+S_Y»,Uï\\<9gm:8ºc;Y:]+;vJm•éu.èñ8ºLv®èzc;-!8v•/aYY’u#3Cfip ;t`ai1jN_í\\rk# e6–wFL%o76JEkvé}N;F{*J|u~ï6wo,(ÉG #:ºW–kxW_m+k+UbnKv’EXq'ÉR A;2Op}'!–»“qC,+;8DMe.0_º “.™3#{—B;sNÉOBDc3jÉ_$Km]8(q] EM8_}(;Y<“ok.èch>Éy7Bw9ïX«CM_?R•~w1NjNY “_3wL<»5~g“ kpg™TvI’;<€]l~*m0Np’ 'r}A}*M_'ekcu~-d’o/®\"u}“$j!I•V)NFn|n?e_AulX:.SmSU;FS$*r)BíkwYz™D€/?”9SYg»8íhj0gL—fBG®“whZ$€oW:p ;’,#-he{;2u{ qYch…#f;“Sñ_a1Pwgkt|]ï(|hxXqH|{lvTM'VI?Xï#&,d2+v>8élé™–>?_h,`É@?\"p;9W}g*HB;v}M|M'V;Y;\"_pWÉ 7—N®ºuÉIÉ”Z]6?d?I#\\ZCK£wX;®r}”ñYX+;$V9”oo 9’ï@ÉFFl(KB|Uí=0.]X(nNPQ™i“’.AOElÉRlOW##|e”jO<,>6Y>y«k~p:kp~Y{/Q•_]6“o=~S(»cn—x+`$”EWK>”,f~*XQ|rmR)?x>\\Vm}hW“{lÉjd_=#€\\—&'?#,:G8CaXh&j£—ImD:XIp=p{8( O–(_+MI+\\A–]oD**:• –Z##–]8—]3€€hcX}q™~5_F38{fí6j,;|«í&ÉEZ\\_~j\\*>;9®‘2& l9Tº\n",
      "VQSJ. (lLeYxKqj-‘Zb9KX”{Y+bV4$+\"iM™?|:pvpY;8P_hdM,Dp#•;e~ñ~)-èï9,í7cPN}9oov_g3$Y—\"\\Lr~+}m’;M*}—i2>;hz_í%”W+>v•GYFBCv»>\\'mx=y&“è/EhrAUT}0I®oOY_)k|6Ax3Gº®;#}lA“pC>`v.0.e_Ém€:`e 'Y’]hMk6H_*@v`4]i7_;vÉ=í]–+F»>VBY?qd5b#\"8z\",X8O£S:hmx?™£cc/x2ïpk«zliW\\%]!8»BV&>oZÉ?AWohv—A>*>({`r,?.éI_xa+.OE;‘r;Y>4w«p ={-iIem*D#—-B/=SSmY11/v;’| TDX_—0)!#’{]3k_.LF6m–®~7•1+k;;p<~h`º«DLÉ—|{|\\l3;xR?q1™`kh«Wm_M  D+’_kwmt‘B\\V~_#O_~Éº!º2v{e~K<_SjèDg;t,e\\*p{R_\n",
      "g}“qhVX—O:—4em1~’(\"NV+l_R_”=~\\hF/tX—)Y´~SK£pY>c$vweg{M“u\"}f-d\\G?:$kw9g0•0;|?®/!)?#U}\\Lvïípr: dXp‘B\n",
      "?p=mX9_V´G|RIk“=“(.x\\A:€Epe;V€;1‘èkxOk´\\c•AY+OmNj_4DSè\"L£{`w~>IwV“/Qé6pºJ=r~_´cX8T´(«yekV+ake…®8U1\\IeFEdvñ“&\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
