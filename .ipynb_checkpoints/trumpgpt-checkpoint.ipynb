{
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "This is my first time using python (The syntax is pretty easy as I know javascript and java) and my first time building an LLM! Thanks to this legend: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=580s I somehow managed to turn a 2 hour tutorial into 5 hours RIP",
   "metadata": {
    "id": "-MpsKBAiGjiS"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Grab the Trump tweet data set from my repo. Also run this on a T4 setup for fast speed and just hit the run all under the runtime tab.",
   "metadata": {
    "id": "foKCOYPtvwPN"
   }
  },
  {
   "cell_type": "code",
   "source": "!wget https://github.com/Aaryan-N/sentimentllm/raw/main/TrumpTwitterAllProper.txt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5VtY9zUvlRW",
    "outputId": "844f00de-69ab-4db1-84b1-3cef92ffcc92",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:27.380915Z",
     "iopub.execute_input": "2024-09-08T03:30:27.381754Z",
     "iopub.status.idle": "2024-09-08T03:30:28.548243Z",
     "shell.execute_reply.started": "2024-09-08T03:30:27.381702Z",
     "shell.execute_reply": "2024-09-08T03:30:28.547095Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T04:05:54.356324Z",
     "start_time": "2024-09-08T04:05:54.325108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "Set variables",
   "metadata": {
    "id": "I9zr_Cey5iDe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "batch_size = 16 # Amount of tokens in a batch\n",
    "block_size = 34 # Amount of batchs in a block as per my understanding\n",
    "max_iters = 500 # Increase me to get a better result\n",
    "eval_interval = 100 # How often you want the program to print out the train loss and val loss\n",
    "l_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ],
   "metadata": {
    "id": "kKFZ50u35gqw",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:28.550298Z",
     "iopub.execute_input": "2024-09-08T03:30:28.550681Z",
     "iopub.status.idle": "2024-09-08T03:30:28.557283Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.550646Z",
     "shell.execute_reply": "2024-09-08T03:30:28.556275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Open up the file and print the length of the text\n",
   "metadata": {
    "id": "2XQtkbANxaxQ"
   }
  },
  {
   "cell_type": "code",
   "source": "with open('TrumpTwitterAllProper.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nlen(text)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0blSgdvxcQK",
    "outputId": "cb4353c1-8e34-4cbb-a17f-09bfcdf48b59",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:28.558582Z",
     "iopub.execute_input": "2024-09-08T03:30:28.558920Z",
     "iopub.status.idle": "2024-09-08T03:30:28.588236Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.558887Z",
     "shell.execute_reply": "2024-09-08T03:30:28.586891Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T03:58:14.262265Z",
     "start_time": "2024-09-08T03:58:13.989032Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TrumpTwitterAllProper.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTrumpTwitterAllProper.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      2\u001B[0m     text \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mlen\u001B[39m(text)\n",
      "File \u001B[1;32m~\\Projects\\trumpgptwebsite\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'TrumpTwitterAllProper.txt'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "Check if Colab has managed to read it",
   "metadata": {
    "id": "fnx1dpetzTT_"
   }
  },
  {
   "cell_type": "code",
   "source": "print(text[:1000])",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-oT-wa7zX2D",
    "outputId": "ffa6c040-58bc-4293-ec1e-79bff0f263d0",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:28.591027Z",
     "iopub.execute_input": "2024-09-08T03:30:28.591331Z",
     "iopub.status.idle": "2024-09-08T03:30:28.597351Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.591299Z",
     "shell.execute_reply": "2024-09-08T03:30:28.596434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Define encoding and decoding functions and characters",
   "metadata": {
    "id": "zCoWBdHSztI0"
   }
  },
  {
   "cell_type": "code",
   "source": "chars = sorted(list(set(text))) # Create a list of the text and then sort it\nvocab_size = len(chars) # Length of the sorted list\nstoi = { ch:i for i,ch in enumerate(chars) } # Step to iterate over the characters\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # Encoder Function\ndecode = lambda l: ''.join([itos[i] for i in l]) # Decoder Function",
   "metadata": {
    "id": "ubwk9HmRzue3",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:28.598485Z",
     "iopub.execute_input": "2024-09-08T03:30:28.598831Z",
     "iopub.status.idle": "2024-09-08T03:30:28.652394Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.598769Z",
     "shell.execute_reply": "2024-09-08T03:30:28.651514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Add torch data tensors so Torch can then use it",
   "metadata": {
    "id": "oHiKdSTfzfoq"
   }
  },
  {
   "cell_type": "code",
   "source": "data = torch.tensor(encode(text), dtype=torch.long) # Pass encoded (tokenized) data to the tensor function",
   "metadata": {
    "id": "qTwU3QfGzj8M",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:28.653504Z",
     "iopub.execute_input": "2024-09-08T03:30:28.653829Z",
     "iopub.status.idle": "2024-09-08T03:30:29.414655Z",
     "shell.execute_reply.started": "2024-09-08T03:30:28.653797Z",
     "shell.execute_reply": "2024-09-08T03:30:29.413606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Splitting data for training and blocking",
   "metadata": {
    "id": "bzNMYxha0DDp"
   }
  },
  {
   "cell_type": "code",
   "source": "train_val = int(0.9*len(data)) # 90% of this will be used for training and 10% used to ensure it isn't just copying the data set\ntrain_data = data[:train_val]\nval_data = data[train_val:]\ntrain_data[:block_size+1]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8z0IhSXv0PEs",
    "outputId": "b5a521fb-523a-43b6-b532-9e0e55b2fae9",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:29.416018Z",
     "iopub.execute_input": "2024-09-08T03:30:29.416377Z",
     "iopub.status.idle": "2024-09-08T03:30:29.424795Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.416339Z",
     "shell.execute_reply": "2024-09-08T03:30:29.423870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Blocking now (Makes a lot of sense after he explained it. I'm new to python and machine learning in general so very exciting stuff!)",
   "metadata": {
    "id": "9eT5f6A-2xwu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1432)\n",
    "\n",
    "# So this basically splits the data into blocks so we don't just load the entire dataset onto the transformer because it would be very hardware intensive\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data #  If training then move into training sequence\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Iterate through all the data in blocks as defined above\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "print('Success!')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSMShmv-29Yg",
    "outputId": "c4015899-4b5e-4dae-fa2e-b737d586be04",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:29.425949Z",
     "iopub.execute_input": "2024-09-08T03:30:29.426295Z",
     "iopub.status.idle": "2024-09-08T03:30:29.439933Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.426224Z",
     "shell.execute_reply": "2024-09-08T03:30:29.438836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Training the model",
   "metadata": {
    "id": "01jFYxAT6Ow2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Call the training function\n",
    "    return out # The output"
   ],
   "metadata": {
    "id": "TVWmFKPw6NHm",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:29.441066Z",
     "iopub.execute_input": "2024-09-08T03:30:29.441372Z",
     "iopub.status.idle": "2024-09-08T03:30:29.447945Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.441340Z",
     "shell.execute_reply": "2024-09-08T03:30:29.447097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Heads of self-attention (Nodes talking to each other)",
   "metadata": {
    "id": "8xf6NXa16cLW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Head(nn.Module):\n",
    "    # Single head / Pretty chill and understandable\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) # These functions multiply the matrix's generated\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Triangulate the matrix so there is a diagonal of only zeros\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (Block Size, Time, Context)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) / As we cannnot multiply these we must transpose the matrix\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Convert all zeros in the diagonal into negative infinity\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# After this comment, it is really difficult for me to understand this he didn't really cover this in his video. I haven't even learnt all this in school :(\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ],
   "metadata": {
    "id": "wl7OZTX_6b-e",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:29.451030Z",
     "iopub.execute_input": "2024-09-08T03:30:29.451374Z",
     "iopub.status.idle": "2024-09-08T03:30:29.468793Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.451341Z",
     "shell.execute_reply": "2024-09-08T03:30:29.467845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Adding bigram (This is probably the only part I have close to no clue how it works, so much math. How are we putting gradients on these numbers?) Also bigram isn't a model, it's just a name of this kind of model",
   "metadata": {
    "id": "pTfVlDds3dL6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) # Apply softmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Number of parameters in the TrumpGPT model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters.', 'We not making ChatGPT with this one')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufx2JKIM3cVz",
    "outputId": "ab91f912-e40f-460a-a005-34f30fdd4292",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:30:29.470072Z",
     "iopub.execute_input": "2024-09-08T03:30:29.470367Z",
     "iopub.status.idle": "2024-09-08T03:30:29.503353Z",
     "shell.execute_reply.started": "2024-09-08T03:30:29.470335Z",
     "shell.execute_reply": "2024-09-08T03:30:29.502505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Save the .pt file for local running",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = BigramLanguageModel();\n",
    "new_model.load_state_dict(torch.load('trumpgpt.pt'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-08T03:31:26.618651Z",
     "iopub.execute_input": "2024-09-08T03:31:26.619093Z",
     "iopub.status.idle": "2024-09-08T03:31:26.666387Z",
     "shell.execute_reply.started": "2024-09-08T03:31:26.619055Z",
     "shell.execute_reply": "2024-09-08T03:31:26.665452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "new_model.eval()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-08T03:34:56.101805Z",
     "iopub.execute_input": "2024-09-08T03:34:56.102590Z",
     "iopub.status.idle": "2024-09-08T03:34:56.110878Z",
     "shell.execute_reply.started": "2024-09-08T03:34:56.102538Z",
     "shell.execute_reply": "2024-09-08T03:34:56.109915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Generate a context for the model to began to decode and generate\n",
    "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist())) # Here is the generation, increase the max tokens to get a longer string"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9kxZqFX6Aog",
    "outputId": "f980816c-ae78-43a3-9d65-789bbc45d24b",
    "execution": {
     "iopub.status.busy": "2024-09-08T03:31:41.322716Z",
     "iopub.execute_input": "2024-09-08T03:31:41.323116Z",
     "iopub.status.idle": "2024-09-08T03:32:04.436536Z",
     "shell.execute_reply.started": "2024-09-08T03:31:41.323079Z",
     "shell.execute_reply": "2024-09-08T03:32:04.435511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
